# Server
HOST=0.0.0.0
PORT=8000
WORKERS=1

# Storage
DATA_DIR=storage
UPLOAD_DIR=storage/uploads

# Database (choose either DATABASE_URL or the discrete params)
# DATABASE_URL=postgresql://USER:PASSWORD@HOST:5432/DBNAME?sslmode=require
DB_HOST=10.10.1.82
DB_PORT=5432
DB_NAME=postgres
DB_USER=postgres
DB_PASSWORD=RAbbithole1234##
DB_SSLMODE=require
DB_POOL_MIN_SIZE=1
DB_POOL_MAX_SIZE=10

# Embeddings
# IMPORTANT: EMBEDDING_DIM must match the chosen model. all-MiniLM-L6-v2 -> 384 dims
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
EMBEDDING_DIM=384
EMBEDDING_BATCH=64

# pgvector index (used only if SEARCH_BACKEND=pgvector)
PGVECTOR_METRIC=cosine
PGVECTOR_LISTS=1000
PGVECTOR_PROBES=25
DB_STORE_EMBEDDINGS=false
# When DB_STORE_EMBEDDINGS=false and SEARCH_BACKEND=opensearch, embeddings are not stored in Postgres.
# Semantic search via pgvector will be disabled; OpenSearch vector search will still work.

# Full-text search config
FTS_CONFIG=english

# Security & Auth
# Basic Auth (fallback for API/testing)
BASIC_AUTH_USER=admin
BASIC_AUTH_PASSWORD=letmein
# Cookie/session settings
APP_NAME=SpacesAI
SECRET_KEY=change-me-please
SESSION_COOKIE_NAME=spacesai_session
SESSION_MAX_AGE_SECONDS=1209600
# Cookies: in production behind TLS, set COOKIE_SECURE=true and consider SameSite=Strict
COOKIE_SECURE=false
COOKIE_SAMESITE=Lax
ALLOW_REGISTRATION=true

# Optional LLM for RAG synthesis
# Set one of: none | openai | oci | bedrock | ollama
LLM_PROVIDER=oci
OPENAI_API_KEY=
OPENAI_MODEL=

# OCI Generative AI (when LLM_PROVIDER=oci)
OCI_REGION=mx-queretaro-1 
OCI_COMPARTMENT_OCID=ocid1.compartment.oc1..aaaaaaaadfdmligjm7aefhatq6n5s2stavjgfq56n7vbnhjpxry7tiqjgmfa
OCI_GENAI_ENDPOINT=https://inference.generativeai.us-chicago-1.oci.oraclecloud.com
## meta.llama-4-scout-17b-16e-instruct Model on OCI GenAI
OCI_GENAI_MODEL_ID=ocid1.generativeaimodel.oc1.us-chicago-1.amaaaaaask7dceyayjawvuonfkw2ua4bob4rlnnlhs522pafbglivtwlfzta
# Option 1: Use config file
OCI_CONFIG_FILE=/home/opc/.oci/config
OCI_CONFIG_PROFILE=DEFAULT
# Option 2: API key envs
OCI_TENANCY_OCID=
OCI_USER_OCID=
OCI_FINGERPRINT=
OCI_PRIVATE_KEY_PATH=
OCI_PRIVATE_KEY_PASSPHRASE=

# CORS
ALLOW_CORS=true
# Comma-separated list of allowed origins for CORS (use * for any during local dev)
CORS_ORIGINS=*

# Upload & PDF parsing
# Maximum size per file (in MB)
MAX_UPLOAD_SIZE_MB=200
# Maximum files per batch (UI/Server enforced)
MAX_UPLOAD_FILES=100
USE_PYMUPDF=true
# Keep uploaded files after ingest to aid debugging; set to true in production if desired
DELETE_UPLOADED_FILES=false

# Storage backend: local | oci | both
STORAGE_BACKEND=oci
# Ingestion CLI respects the same storage settings and writes into storage/uploads/
# OCI Object Storage bucket name for uploads (if STORAGE_BACKEND includes 'oci')
# Objects are stored under <email>/YYYY/MM/DD/HHMMSS/<filename>
OCI_OS_BUCKET_NAME=search-app-uploads
# Toggle OCI Object Storage uploads; when false, files are stored locally only and ingestion/search still proceed
OCI_OS_UPLOAD=true

# Retrieval backend: opensearch | pgvector
SEARCH_BACKEND=opensearch
# OpenSearch is recommended when DB_STORE_EMBEDDINGS=false

# OpenSearch
OPENSEARCH_HOST=https://amaaaaaawiclygaashyteiqunkonejhfurxooxj2vv6ubcrflyruk5oa34tq.opensearch.mx-queretaro-1.oci.oraclecloud.com:9200
OPENSEARCH_INDEX=spacesai_chunks
OPENSEARCH_USER=osmaster
OPENSEARCH_PASSWORD=RAbbithole1234##
OPENSEARCH_TIMEOUT=120
OPENSEARCH_MAX_RETRIES=8
OPENSEARCH_VERIFY_CERTS=true
OPENSEARCH_DUAL_WRITE=true
# Index layout
OPENSEARCH_SHARDS=3
OPENSEARCH_REPLICAS=1
# KNN engine tuning (optional)
# OPENSEARCH_KNN_ENGINE=lucene
# OPENSEARCH_KNN_NUM_CANDIDATES=500  # Used only when engine != lucene; if unset, defaults to max(top_k*10, 100)
# OPENSEARCH_DISTANCE=cosinesimil


# Valkey (Redis-compatible) cache
VALKEY_HOST=aaawiclygaa5mj533ohobwqjuel7arrc5dke6b5vkbebytopurv5uva-p.redis.mx-queretaro-1.oci.oraclecloud.com
VALKEY_PORT=6379
VALKEY_PASSWORD=
VALKEY_DB=0
VALKEY_TLS=true
CACHE_TTL_SECONDS=300

# AWS Bedrock (optional)
AWS_REGION=us-east-1
AWS_BEDROCK_MODEL_ID=anthropic.claude-3-sonnet-20240229-v1:0

# Ollama (optional)
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.2:latest

# Chunking (ingestion)
# Controls chunk size and overlap used for splitting extracted text before embedding
# If CHUNK_AUTO_TUNE=true, the app will adapt chunk size to document structure
CHUNK_SIZE=2500
CHUNK_OVERLAP=250
CHUNK_AUTO_TUNE=true
# Bounds for auto-tuned chunk size
CHUNK_MIN_SIZE=800
CHUNK_MAX_SIZE=3500
# Proportional overlap when auto-tuning (0.0 - 1.0)
CHUNK_OVERLAP_RATIO=0.1